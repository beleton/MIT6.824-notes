---
dg-publish: true
---
GFS论文中文翻译：[大数据 - The Google File System（个人翻译） - 个人文章 - SegmentFault 思否](https://segmentfault.com/a/1190000023309479)
# 1 存储系统

- 高性能(high perference)：
	- shard：分片，并发读取
	- data across server：跨多个服务机器存储数据（单机的网卡、CPU等硬件吞吐量限制）
- 多实例/多机器(many servers)
- 容错(fault tolerance)：
	- 复制(replication)：通常通过复制数据保证容错性，即当前磁盘数据异常/缺失等情况，尝试从另一个磁盘获取数据
- 一致性：​ 理想的一致性，整个分布式系统对外表现时像单个机器一样运作。
# 2 GFS
## 2.1 概述
​ GFS旨在保持高性能，具有高可用性和容错能力。

## 2.2 架构
### 2.1 总体架构
GFS的架构由三个主要组件组成：主服务器、块服务器和客户端。文件被划分为固定大小的块（默认64MB），每个块有一个唯一的64bit块句柄(chunk hundle)，由主控服务器在创建块的时候分配。
下图展示了GFS的基本架构：![[GFS结构.png]]
- **主服务器 (Master Server)**：管理文件系统的元数据，包括命名空间、访问控制信息、从文件到块的映射以及每个块的副本位置。这些元数据被存储在主服务器的内存中，并通过日志进行持久化。
- **块服务器 (Chunk Servers)**：存储实际的数据块，每个块服务器存储多个块，并且每个块在多个块服务器上有副本（默认是三份）。
- **客户端 (Clients)**：通过一个库来访问文件系统，库与主服务器和块服务器进行通信以执行文件操作。

​
### 2.2 单Master
- 优点：
	所有元数据操作都通过Master处理，避免了多Master环境下复杂的同步和一致性问题。
	Master拥有整个文件系统的全局视图，可以做出更优化的决策，例如数据块的分配和负载均衡。

- 缺点：
	- 单点故障风险：一旦Master失效，整个文件系统的元数据操作都将中断。
	- 性能瓶颈：所有元数据请求都集中到一个Master上，可能导致其负载过重，影响系统整体性能。

### 2.3 元数据Metadata
元数据存储在主内存中，而数据存储在块服务器中。这使得主服务器操作非常快，并且还允许主服务器在后台通过其整个状态有效地执行定期扫描。周期性扫描用于实现块垃圾收集、块迁移等。
master节点主要存储三种类型的元数据：
- 文件和chunk的命名空间(namespace)
- 文件和chunk的映射关系(mapping)
- 每个chunk副本的位置(location)

### 2.4 文件操作
**文件读取流程**
 GFS通过Master管理文件系统的元数据等信息。应用通过GFS Client读取数据时，大致流程如下
1. **客户端请求文件块位置**：
    - 客户端向主服务器（Master）发送请求，要求获取特定文件中某个偏移量（offset）对应的数据块（chunk）的位置。
    - 主服务器查找该文件块的元数据（metadata），包括块句柄（chunk handle）和存储该块的块服务器（Chunk Servers）列表。
    - 主服务器将该数据块的句柄及其副本所在的块服务器地址列表返回给客户端。客户端将此信息缓存（cache），以便在后续的读取操作中减少与主服务器的交互。
2. **客户端向块服务器请求数据**：
    
    - 客户端根据主服务器返回的块服务器列表，选择一个块服务器（通常选择最近的块服务器）并直接发送数据读取请求。
    - 客户端的请求包含块句柄（chunk handle）和读取的字节范围（byte range）。
3. **块服务器返回数据**：
    
    - 块服务器接收到读取请求后，读取指定的数据块范围，并将数据返回给客户端。
    - 客户端接收数据后，处理并返回给应用程序。


**文件追加流程**：
![[文件追加流程.png]]
1. **客户端向主服务器查询写入位置**：
    - 客户端向主服务器（Master）发出请求，查询应该往哪写入文件（filename）对应的数据。
    - 主服务器查询文件名到块句柄（chunk handle）映射表，找到需要修改的块句柄后，再查询块句柄到块服务器（chunk servers）数组的映射表，返回包含主副本（primary）、次副本（secondaries）及版本信息的块服务器列表作为响应结果。
2. **确定主副本（primary）**：
    - 如果已有主副本，继续后续流程。
    - 如果没有主副本（例如系统刚启动不久，还没有主副本），主服务器会从块服务器中选出一个作为主副本，其余的作为次副本。主服务器会增加版本号（version），并向主副本和次副本发送新的版本信息及租约（lease）。主副本和次副本需要将版本信息存储到磁盘，以确保重启后版本信息不丢失。
3. **客户端发送数据**：
    - 客户端将数据发送到所有需要写入的块服务器（包括主副本和次副本）。为了提高吞吐量，客户端只需访问最近的次副本，次副本会将数据转发给列表中的下一个块服务器。此时数据还不会真正被块服务器存储（即中间黑色粗箭头所示，次副本收到数据后，立即将数据推送到其他需要写入的块服务器）。
    这样做的目的是提高客户端的吞吐量，避免客户端本身需要消耗大量网络资源同时向主副本和多个次副本发送数据。
4. **客户端通知主副本**：
    - 数据传递完毕后，客户端向主副本发送一个消息，表明这是一次追加（append）操作。
5. **主副本处理数据**：主副本需要做以下几件事：
    - 检查版本信息是否匹配，如果不匹配，则拒绝客户端的操作。
    - 检查租约是否仍然有效，如果租约无效，则不再接受任何变更操作（mutation operations），因为可能已经存在一个新的主副本。
    - 如果版本和租约都有效，主副本会选择一个偏移量（offset）用于写入数据。
    - 主副本将前面接收到的数据写入稳定存储中。
    - 主副本向次副本发送消息，指示它们将之前接收的数据写入指定的偏移量。
6. **次副本写入数据**：
    - 次副本将数据写入到主副本指定的偏移量，并回应主副本已完成数据写入。
7. **主副本回应客户端**：
    - 主副本回应客户端，确认追加的数据已完成写入。

### 2.5 一致性
GFS采用了一种松散的一致性模型，旨在平衡性能和一致性，确保在高分布式环境下的可扩展性和可靠性。
#### 1. **一致性保证**
- **文件命名空间变更的原子性**：文件命名空间的变更（例如文件创建、删除、重命名等）是原子的。所有命名空间的变更操作都由主服务器处理，通过命名空间锁来保证其原子性和正确性。
- **数据变更的一致性**：数据变更包括写操作和记录追加操作（record append）。GFS保证在所有副本上按照相同的顺序应用变更，以确保一致性。
#### 2. **数据变更类型**
- **写操作（Write Operation）**：写操作将数据写入到文件中的指定偏移量。成功的写操作在所有副本上以相同的顺序应用，使得该区域在所有副本上都是一致的。
- **记录追加（Record Append）**：记录追加操作会将数据追加到文件的末尾，并返回实际写入的偏移量。即使在多个客户端同时追加数据的情况下，GFS也能保证每个追加操作是原子的。
#### 3. **一致性状态**
- **一致（Consistent）**：所有客户端在读取同一区域时，都会看到相同的数据。
- **定义（Defined）**：一个区域的一致性且所有客户端都会看到完整的写入数据。当没有并发写入操作时，写入操作成功的区域是定义的。
- **未定义（Undefined）**：并发写入操作的区域是未定义的，意味着数据可能是多个写入操作混合的结果。尽管如此，所有客户端看到的未定义区域数据是一致的。
- **不一致（Inconsistent）**：写入操作失败的区域是不一致的，不同客户端在读取该区域时可能会看到不同的数据。
#### 4. **数据一致性维护**
- **租约机制（Lease Mechanism）**：主服务器通过授予一个块的主副本（primary）租约来维持一致性。主副本负责序列化所有的变更操作，并将其顺序应用到次副本（secondary）。
- **块版本号（Chunk Version Number）**：每个数据块有一个版本号，主服务器在授予新的主副本租约时会增加版本号。版本号用于检测陈旧副本（stale replicas），确保只有最新的数据副本被客户端读取。
#### 5. **松散一致性的应用**
- **追加操作优化**：大多数GFS的应用程序通过追加而不是覆盖来修改文件。追加操作效率高，且对应用故障更具恢复力。
- **检查点和验证**：应用程序可以通过定期创建检查点（checkpoint）和使用校验和（checksum）来验证数据完整性，确保在读取数据时只处理已定义的区域。
#### 6. **读取操作中的一致性**
- **读取过时数据（Stale Data Reads）**：由于客户端缓存了块位置信息，可能会在短时间内读取到过时的数据。为了减小这种情况的影响，GFS设置了缓存的超时时间，并在文件重新打开时刷新缓存。